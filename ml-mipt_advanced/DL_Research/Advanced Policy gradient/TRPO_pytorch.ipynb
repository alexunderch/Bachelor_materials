{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TRPO_pytorch.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/master/week09_policy_II/seminar_TRPO_pytorch.ipynb","timestamp":1633769854953}],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZKH3zuALha_0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633769897236,"user_tz":-180,"elapsed":16383,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}},"outputId":"61eca0a7-688d-47cf-b710-e9fac79f7148"},"source":["import sys\n","if 'google.colab' in sys.modules:\n","    import os\n","\n","    os.system('apt-get install -y xvfb')\n","    os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n","    os.system('apt-get install -y python-opengl ffmpeg')\n","    os.system('pip install pyglet==1.2.4')\n","\n","# launch XVFB if you run on a server\n","import os\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting virtual X frame buffer: Xvfb.\n"]}]},{"cell_type":"markdown","metadata":{"id":"3xfVHmBWha_5"},"source":["### Let's make a TRPO!\n","\n","In this notebook we will write the code of the one Trust Region Policy Optimization.\n","As usually, it contains a few different parts which we are going to reproduce.\n","\n"]},{"cell_type":"code","metadata":{"id":"h5BFJWIIha_7","executionInfo":{"status":"ok","timestamp":1633769924404,"user_tz":-180,"elapsed":27174,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"TmxvbUU6ha_8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633769971212,"user_tz":-180,"elapsed":988,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}},"outputId":"d8cca176-2ed3-4e5c-8cd8-0b3e9916f0d2"},"source":["import gym\n","\n","env = gym.make(\"Acrobot-v1\")\n","env.reset()\n","observation_shape = env.observation_space.shape\n","n_actions = env.action_space.n\n","print(\"Observation Space\", env.observation_space)\n","print(\"Action Space\", env.action_space)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Observation Space Box(-28.274333953857422, 28.274333953857422, (6,), float32)\n","Action Space Discrete(3)\n"]}]},{"cell_type":"code","metadata":{"id":"yxKYOQ9Jha_9"},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","render = lambda : plt.imshow(env.render(mode='rgb_array'))\n","env.reset()\n","render()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mFgEwm5Lha_9"},"source":["### Step 1: Defining a network\n","\n","With all it's complexity, at it's core TRPO is yet another policy gradient method. \n","\n","This essentially means we're actually training a stochastic policy $ \\pi_\\theta(a|s) $. \n","\n","And yes, it's gonna be a neural network. So let's start by defining one."]},{"cell_type":"code","metadata":{"id":"deHaWvI9ha_-","executionInfo":{"status":"ok","timestamp":1633770485253,"user_tz":-180,"elapsed":396,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["class TRPOAgent(nn.Module):\n","    def __init__(self, state_shape, n_actions, hidden_size=32):\n","        '''\n","        Here you should define your model\n","        You should have LOG-PROBABILITIES as output because you will need it to compute loss\n","        We recommend that you start simple: \n","        use 1-2 hidden layers with 100-500 units and relu for the first try\n","        '''\n","        nn.Module.__init__(self)\n","\n","        \n","        self.model = nn.Sequential(nn.Linear(state_shape[0], hidden_size), nn.ReLU(),\n","                                   nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n","                                   nn.Linear(hidden_size, n_actions), nn.LogSoftmax())\n","\n","    def forward(self, states):\n","        \"\"\"\n","        takes agent's observation (Variable), returns log-probabilities (Variable)\n","        :param state_t: a batch of states, shape = [batch_size, state_shape]\n","        \"\"\"\n","\n","        # Use your network to compute log_probs for given state\n","        log_probs = self.model(states)\n","        return log_probs\n","\n","    def get_log_probs(self, states):\n","        '''\n","        Log-probs for training\n","        '''\n","\n","        return self.forward(states)\n","\n","    def get_probs(self, states):\n","        '''\n","        Probs for interaction\n","        '''\n","\n","        return torch.exp(self.forward(states))\n","\n","    def act(self, obs, sample=True):\n","        '''\n","        Samples action from policy distribution (sample = True) or takes most likely action (sample = False)\n","        :param: obs - single observation vector\n","        :param sample: if True, samples from \\pi, otherwise takes most likely action\n","        :returns: action (single integer) and probabilities for all actions\n","        '''\n","\n","        probs = self.get_probs(Variable(torch.FloatTensor([obs]))).data.numpy()\n","\n","        if sample:\n","            action = int(np.random.choice(n_actions, p=probs[0]))\n","        else:\n","            action = int(np.argmax(probs))\n","\n","        return action, probs[0]\n","\n","\n","agent = TRPOAgent(observation_shape, n_actions)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlczE-Qqha__","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633770488050,"user_tz":-180,"elapsed":308,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}},"outputId":"04be4df7-41c5-4e3c-fcaf-b61cdebc771d"},"source":["# Check if log-probabilities satisfies all the requirements\n","log_probs = agent.get_log_probs(Variable(torch.FloatTensor([env.reset()])))\n","assert isinstance(\n","    log_probs, Variable) and log_probs.requires_grad, \"qvalues must be a torch variable with grad\"\n","assert len(\n","    log_probs.shape) == 2 and log_probs.shape[0] == 1 and log_probs.shape[1] == n_actions\n","sums = torch.sum(torch.exp(log_probs), dim=1)\n","assert (0.999 < sums).all() and (1.001 > sums).all()\n","\n","# Demo use\n","print(\"sampled:\", [agent.act(env.reset()) for _ in range(5)])\n","print(\"greedy:\", [agent.act(env.reset(), sample=False) for _ in range(5)])"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["sampled: [(2, array([0.3256434 , 0.34741598, 0.32694063], dtype=float32)), (2, array([0.3266239 , 0.3488253 , 0.32455075], dtype=float32)), (2, array([0.32540736, 0.34913456, 0.32545808], dtype=float32)), (2, array([0.32523468, 0.3478601 , 0.32690522], dtype=float32)), (2, array([0.3267628 , 0.34747267, 0.32576463], dtype=float32))]\n","greedy: [(1, array([0.32528457, 0.34707054, 0.32764485], dtype=float32)), (1, array([0.32703638, 0.3458912 , 0.32707244], dtype=float32)), (1, array([0.3276196 , 0.34574807, 0.3266323 ], dtype=float32)), (1, array([0.32792926, 0.34695294, 0.3251178 ], dtype=float32)), (1, array([0.32572532, 0.34647512, 0.32779962], dtype=float32))]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"]}]},{"cell_type":"markdown","metadata":{"id":"WY3z11SkhbAA"},"source":["#### Flat parameters operations\n","\n","We are going to use it"]},{"cell_type":"code","metadata":{"id":"GIFIvynQhbAA","executionInfo":{"status":"ok","timestamp":1633770535557,"user_tz":-180,"elapsed":424,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["def get_flat_params_from(model):\n","    params = []\n","    for param in model.parameters():\n","        params.append(param.data.view(-1))\n","\n","    flat_params = torch.cat(params)\n","    return flat_params\n","\n","\n","def set_flat_params_to(model, flat_params):\n","    prev_ind = 0\n","    for param in model.parameters():\n","        flat_size = int(np.prod(list(param.size())))\n","        param.data.copy_(\n","            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n","        prev_ind += flat_size"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GeB5KvMGhbAB"},"source":["Compute cummulative reward just like you did in vanilla REINFORCE"]},{"cell_type":"code","metadata":{"id":"1vcmOYSBhbAB","executionInfo":{"status":"ok","timestamp":1633770542671,"user_tz":-180,"elapsed":678,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["import scipy.signal\n","\n","\n","def get_cummulative_returns(r, gamma=1):\n","    \"\"\"\n","    Computes cummulative discounted rewards given immediate rewards\n","    G_i = r_i + gamma*r_{i+1} + gamma^2*r_{i+2} + ...\n","    Also known as R(s,a).\n","    \"\"\"\n","    r = np.array(r)\n","    assert r.ndim >= 1\n","    return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"4O20bEOFhbAC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633770543939,"user_tz":-180,"elapsed":302,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}},"outputId":"772fbfe0-c4d5-40c6-c03a-535e53c20235"},"source":["# simple demo on rewards [0,0,1,0,0,1]\n","get_cummulative_returns([0, 0, 1, 0, 0, 1], gamma=0.9)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.40049, 1.5561 , 1.729  , 0.81   , 0.9    , 1.     ])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"_3k7IE_ChbAC"},"source":["**Rollout**"]},{"cell_type":"code","metadata":{"id":"G3OuWMoAhbAE","executionInfo":{"status":"ok","timestamp":1633770579595,"user_tz":-180,"elapsed":323,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["def rollout(env, agent, max_pathlength=2500, n_timesteps=50000):\n","    \"\"\"\n","    Generate rollouts for training.\n","    :param: env - environment in which we will make actions to generate rollouts.\n","    :param: act - the function that can return policy and action given observation.\n","    :param: max_pathlength - maximum size of one path that we generate.\n","    :param: n_timesteps - total sum of sizes of all pathes we generate.\n","    \"\"\"\n","    paths = []\n","\n","    total_timesteps = 0\n","    while total_timesteps < n_timesteps:\n","        obervations, actions, rewards, action_probs = [], [], [], []\n","        obervation = env.reset()\n","        for _ in range(max_pathlength):\n","            action, policy = agent.act(obervation)\n","            obervations.append(obervation)\n","            actions.append(action)\n","            action_probs.append(policy)\n","            obervation, reward, done, _ = env.step(action)\n","            rewards.append(reward)\n","            total_timesteps += 1\n","            if done or total_timesteps == n_timesteps:\n","                path = {\"observations\": np.array(obervations),\n","                        \"policy\": np.array(action_probs),\n","                        \"actions\": np.array(actions),\n","                        \"rewards\": np.array(rewards),\n","                        \"cumulative_returns\": get_cummulative_returns(rewards),\n","                        }\n","                paths.append(path)\n","                break\n","    return paths"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-BZrbzBhbAF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633770587168,"user_tz":-180,"elapsed":282,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}},"outputId":"14865bdb-c16c-41de-e68d-5e16e0b7150b"},"source":["paths = rollout(env, agent, max_pathlength=5, n_timesteps=100)\n","print(paths[-1])\n","assert (paths[0]['policy'].shape == (5, n_actions))\n","assert (paths[0]['cumulative_returns'].shape == (5,))\n","assert (paths[0]['rewards'].shape == (5,))\n","assert (paths[0]['observations'].shape == (5,)+observation_shape)\n","assert (paths[0]['actions'].shape == (5,))\n","print('It\\'s ok')"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["{'observations': array([[ 0.99572723, -0.09234332,  0.99937345,  0.03539366,  0.07114725,\n","         0.0929847 ],\n","       [ 0.99679383, -0.08001283,  0.99711871,  0.07585692,  0.05152212,\n","         0.30692856],\n","       [ 0.99896711, -0.04543916,  0.9962998 ,  0.085946  ,  0.28781252,\n","        -0.20149738],\n","       [ 0.99955518,  0.02982345,  0.99999868,  0.00162741,  0.44692456,\n","        -0.61615123],\n","       [ 0.99227721,  0.12404006,  0.98935893, -0.14549536,  0.47439957,\n","        -0.81945233]]), 'policy': array([[0.32543394, 0.34701785, 0.32754818],\n","       [0.3250749 , 0.34429917, 0.33062592],\n","       [0.32693565, 0.35169825, 0.3213661 ],\n","       [0.333035  , 0.35620865, 0.31075633],\n","       [0.33779347, 0.35701343, 0.305193  ]], dtype=float32), 'actions': array([2, 0, 0, 0, 0]), 'rewards': array([-1., -1., -1., -1., -1.]), 'cumulative_returns': array([-5., -4., -3., -2., -1.])}\n","It's ok\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"]}]},{"cell_type":"markdown","metadata":{"id":"ViWiBioUhbAF"},"source":["### Step 3: Auxiliary functions\n","\n","Now let's define the loss functions and something else for actual TRPO training."]},{"cell_type":"markdown","metadata":{"id":"lOVnDAVehbAF"},"source":["The surrogate reward should be\n","$$J_{surr}= {1 \\over N} \\sum\\limits_{i=0}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}A_{\\theta_{old}(s_i, a_i)}$$\n","\n","For simplicity, let's use cummulative returns instead of advantage for now:\n","$$J'_{surr}= {1 \\over N} \\sum\\limits_{i=0}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}G_{\\theta_{old}(s_i, a_i)}$$\n","\n","Or alternatively, minimize the surrogate loss:\n","$$ L_{surr} = - J'_{surr} $$  \n"]},{"cell_type":"code","metadata":{"id":"mEjQ__GfhbAF","executionInfo":{"status":"ok","timestamp":1633771548093,"user_tz":-180,"elapsed":409,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["def get_loss(agent, observations, actions, cummulative_returns, old_probs):\n","    \"\"\"\n","    Computes TRPO objective\n","    :param: observations - batch of observations\n","    :param: actions - batch of actions\n","    :param: cummulative_returns - batch of cummulative returns\n","    :param: old_probs - batch of probabilities computed by old network\n","    :returns: scalar value of the objective function\n","    \"\"\"\n","    batch_size = observations.shape[0]\n","    log_probs_all = agent.get_log_probs(observations)\n","    probs_all = torch.exp(log_probs_all)\n","\n","    probs_for_actions = probs_all[torch.arange(\n","        0, batch_size, out=torch.LongTensor()), actions]\n","    old_probs_for_actions = old_probs[torch.arange(\n","        0, batch_size, out=torch.LongTensor()), actions]\n","\n","    # Compute surrogate loss, aka importance-sampled policy gradient\n","    Loss = -torch.mean(cummulative_returns * (probs_for_actions / old_probs_for_actions))\n","    assert Loss.shape == torch.Size([])\n","    return Loss"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f7kIlM9JhbAG"},"source":["We can ascend these gradients as long as our $pi_\\theta(a|s)$ satisfies the constraint\n","$$E_{s,\\pi_{\\Theta_{t}}}\\Big[KL(\\pi(\\Theta_{t}, s) \\:||\\:\\pi(\\Theta_{t+1}, s))\\Big]< \\alpha$$\n","\n","\n","where\n","\n","$$KL(p||q) = E _p log({p \\over q})$$"]},{"cell_type":"code","metadata":{"id":"VkKwNfV-hbAG","executionInfo":{"status":"ok","timestamp":1633771596742,"user_tz":-180,"elapsed":311,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["def get_kl(agent, observations, actions, cummulative_returns, old_probs_all):\n","    \"\"\"\n","    Computes KL-divergence between network policy and old policy\n","    :param: observations - batch of observations\n","    :param: actions - batch of actions\n","    :param: cummulative_returns - batch of cummulative returns (we don't need it actually)\n","    :param: old_probs - batch of probabilities computed by old network\n","    :returns: scalar value of the KL-divergence\n","    \"\"\"\n","    batch_size = observations.shape[0]\n","    log_probs_all = agent.get_log_probs(observations)\n","    probs_all = torch.exp(log_probs_all)\n","\n","    # Compute Kullback-Leibler divergence (see formula above)\n","    # Note: you need to sum KL and entropy over all actions, not just the ones agent took\n","    old_log_probs = torch.log(old_probs_all+1e-10)\n","\n","    kl = torch.sum(old_probs_all * (-log_probs_all + old_log_probs)) / batch_size\n","\n","    assert kl.shape == torch.Size([])\n","    assert (kl > -0.0001).all() and (kl < 10000).all()\n","    return kl"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVju12eThbAG","executionInfo":{"status":"ok","timestamp":1633770996266,"user_tz":-180,"elapsed":9,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["def get_entropy(agent, observations):\n","    \"\"\"\n","    Computes entropy of the network policy \n","    :param: observations - batch of observations\n","    :returns: scalar value of the entropy\n","    \"\"\"\n","\n","    observations = Variable(torch.FloatTensor(observations))\n","\n","    batch_size = observations.shape[0]\n","    log_probs_all = agent.get_log_probs(observations)\n","    probs_all = torch.exp(log_probs_all)\n","\n","    entropy = torch.sum(-probs_all * log_probs_all) / batch_size\n","\n","    assert entropy.shape == torch.Size([])\n","    return entropy"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hX2sSy5WhbAH"},"source":["**Linear search**\n","\n","TRPO in its core involves ascending surrogate policy gradient constrained by KL divergence. \n","\n","In order to enforce this constraint, we're gonna use linesearch. You can find out more about it [here](https://en.wikipedia.org/wiki/Linear_search)"]},{"cell_type":"code","metadata":{"id":"iwrigF0-hbAH","executionInfo":{"status":"ok","timestamp":1633771113163,"user_tz":-180,"elapsed":337,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["def linesearch(f, x, fullstep, max_kl):\n","    \"\"\"\n","    Linesearch finds the best parameters of neural networks in the direction of fullstep contrainted by KL divergence.\n","    :param: f - function that returns loss, kl and arbitrary third component.\n","    :param: x - old parameters of neural network.\n","    :param: fullstep - direction in which we make search.\n","    :param: max_kl - constraint of KL divergence.\n","    :returns:\n","    \"\"\"\n","    max_backtracks = 10\n","    loss, _, = f(x)\n","    for stepfrac in .5**np.arange(max_backtracks):\n","        xnew = x + stepfrac * fullstep\n","        new_loss, kl = f(xnew)\n","        actual_improve = new_loss - loss\n","        if kl.data.numpy() <= max_kl and actual_improve.data.numpy() < 0:\n","            x = xnew\n","            loss = new_loss\n","    return x"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LLTvik17hbAH"},"source":["**Conjugate gradients**\n","\n","Since TRPO includes contrainted optimization, we will need to solve Ax=b using conjugate gradients.\n","\n","In general, CG is an algorithm that solves Ax=b where A is positive-defined. A is Hessian matrix so A is positive-defined. You can find out more about them [here](https://en.wikipedia.org/wiki/Conjugate_gradient_method)"]},{"cell_type":"code","metadata":{"id":"55FQ0h8khbAI","executionInfo":{"status":"ok","timestamp":1633771113500,"user_tz":-180,"elapsed":2,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["from numpy.linalg import inv\n","\n","\n","def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n","    \"\"\"\n","    This method solves system of equation Ax=b using iterative method called conjugate gradients\n","    :f_Ax: function that returns Ax\n","    :b: targets for Ax\n","    :cg_iters: how many iterations this method should do\n","    :residual_tol: epsilon for stability\n","    \"\"\"\n","    p = b.clone()\n","    r = b.clone()\n","    x = torch.zeros(b.size())\n","    rdotr = torch.sum(r*r)\n","    for i in range(cg_iters):\n","        z = f_Ax(p)\n","        v = rdotr / (torch.sum(p*z) + 1e-8)\n","        x += v * p\n","        r -= v * z\n","        newrdotr = torch.sum(r*r)\n","        mu = newrdotr / (rdotr + 1e-8)\n","        p = r + mu * p\n","        rdotr = newrdotr\n","        if rdotr < residual_tol:\n","            break\n","    return x"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7HIcJoVhbAI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633771120682,"user_tz":-180,"elapsed":322,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}},"outputId":"7923c35c-3269-4add-c26f-adfd6f626e4c"},"source":["# This code validates conjugate gradients\n","A = np.random.rand(8, 8)\n","A = np.matmul(np.transpose(A), A)\n","\n","\n","def f_Ax(x):\n","    return torch.matmul(torch.FloatTensor(A), x.view((-1, 1))).view(-1)\n","\n","\n","b = np.random.rand(8)\n","\n","w = np.matmul(np.matmul(inv(np.matmul(np.transpose(A), A)),\n","                        np.transpose(A)), b.reshape((-1, 1))).reshape(-1)\n","print(w)\n","print(conjugate_gradient(f_Ax, torch.FloatTensor(b)).numpy())"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[-7.90757992  0.56310388  5.72950812 -4.12715387  1.29772852  5.54998255\n"," -2.44521273  0.07863033]\n","[-7.900206    0.5626282   5.7296047  -4.123742    1.2956073   5.5422926\n"," -2.4357243   0.07319292]\n"]}]},{"cell_type":"markdown","metadata":{"id":"x2oLd93QhbAI"},"source":["### Step 4: training\n","In this section we construct the whole update step function."]},{"cell_type":"code","metadata":{"id":"DWc8EoTLhbAJ","executionInfo":{"status":"ok","timestamp":1633771475703,"user_tz":-180,"elapsed":328,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}}},"source":["def update_step(agent, observations, actions, cummulative_returns, old_probs, max_kl):\n","    \"\"\"\n","    This function does the TRPO update step\n","    :param: observations - batch of observations\n","    :param: actions - batch of actions\n","    :param: cummulative_returns - batch of cummulative returns\n","    :param: old_probs - batch of probabilities computed by old network\n","    :param: max_kl - controls how big KL divergence may be between old and new policy every step.\n","    :returns: KL between new and old policies and the value of the loss function.\n","    \"\"\"\n","\n","    # Here we prepare the information\n","    observations = Variable(torch.FloatTensor(observations))\n","    actions = torch.LongTensor(actions)\n","    cummulative_returns = Variable(torch.FloatTensor(cummulative_returns))\n","    old_probs = Variable(torch.FloatTensor(old_probs))\n","\n","    # Here we compute gradient of the loss function\n","    loss = get_loss(agent, observations, actions,\n","                    cummulative_returns, old_probs)\n","    grads = torch.autograd.grad(loss, agent.parameters())\n","    loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n","\n","    def Fvp(v):\n","        # Here we compute Fx to do solve Fx = g using conjugate gradients\n","        # We actually do here a couple of tricks to compute it efficiently\n","\n","        kl = get_kl(agent, observations, actions,\n","                    cummulative_returns, old_probs)\n","\n","        grads = torch.autograd.grad(kl, agent.parameters(), create_graph=True)\n","        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n","\n","        kl_v = (flat_grad_kl * Variable(v)).sum()\n","        grads = torch.autograd.grad(kl_v, agent.parameters())\n","        flat_grad_grad_kl = torch.cat(\n","            [grad.contiguous().view(-1) for grad in grads]).data\n","\n","        return flat_grad_grad_kl + v * 0.1\n","\n","    # Here we solveolve Fx = g system using conjugate gradients\n","    stepdir = conjugate_gradient(Fvp, -loss_grad, 10)\n","\n","    # Here we compute the initial vector to do linear search\n","    shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n","\n","    lm = torch.sqrt(shs / max_kl)\n","    fullstep = stepdir / lm[0]\n","\n","    neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n","\n","    # Here we get the start point\n","    prev_params = get_flat_params_from(agent)\n","\n","    def get_loss_kl(params):\n","        # Helper for linear search\n","        set_flat_params_to(agent, params)\n","        return [get_loss(agent, observations, actions, cummulative_returns, old_probs),\n","                get_kl(agent, observations, actions, cummulative_returns, old_probs)]\n","\n","    # Here we find our new parameters\n","    new_params = linesearch(get_loss_kl, prev_params, fullstep, max_kl)\n","\n","    # And we set it to our network\n","    set_flat_params_to(agent, new_params)\n","\n","    return get_loss_kl(new_params)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5d5pkkpNhbAJ"},"source":["##### Step 5: Main TRPO loop\n","\n","Here we will train our network!"]},{"cell_type":"code","metadata":{"id":"AlKslOSYhbAK","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1633771943822,"user_tz":-180,"elapsed":343399,"user":{"displayName":"Александр Сергеевич Чернявский","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjfde4jD6kZAd2KHSZWkMk63eF8NAzhqhSkasLVqg=s64","userId":"07399096020997671028"}},"outputId":"46d03dc0-8ed9-4eaf-d030-62c50f5219f1"},"source":["import time\n","from itertools import count\n","from collections import OrderedDict\n","\n","# this is hyperparameter of TRPO. It controls how big KL divergence may be between old and new policy every step.\n","max_kl = 0.01\n","numeptotal = 0  # this is number of episodes that we played.\n","\n","start_time = time.time()\n","\n","for i in count(1):\n","\n","    print(\"\\n********** Iteration %i ************\" % i)\n","\n","    # Generating paths.\n","    print(\"Rollout\")\n","    paths = rollout(env, agent)\n","    print(\"Made rollout\")\n","\n","    # Updating policy.\n","    observations = np.concatenate([path[\"observations\"] for path in paths])\n","    actions = np.concatenate([path[\"actions\"] for path in paths])\n","    returns = np.concatenate([path[\"cumulative_returns\"] for path in paths])\n","    old_probs = np.concatenate([path[\"policy\"] for path in paths])\n","\n","    loss, kl = update_step(agent, observations, actions,\n","                           returns, old_probs, max_kl)\n","\n","    # Report current progress\n","    episode_rewards = np.array([path[\"rewards\"].sum() for path in paths])\n","\n","    stats = OrderedDict()\n","    numeptotal += len(episode_rewards)\n","    stats[\"Total number of episodes\"] = numeptotal\n","    stats[\"Average sum of rewards per episode\"] = episode_rewards.mean()\n","    stats[\"Std of rewards per episode\"] = episode_rewards.std()\n","    stats[\"Time elapsed\"] = \"%.2f mins\" % ((time.time() - start_time)/60.)\n","    stats[\"KL between old and new distribution\"] = kl.data.numpy()\n","    stats[\"Entropy\"] = get_entropy(agent, observations).data.numpy()\n","    stats[\"Surrogate loss\"] = loss.data.numpy()\n","    for k, v in stats.items():\n","        print(k + \": \" + \" \" * (40 - len(k)) + str(v))\n","    i += 1"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","********** Iteration 1 ************\n","Rollout\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"]},{"output_type":"stream","name":"stdout","text":["Made rollout\n","Total number of episodes:                 101\n","Average sum of rewards per episode:       -495.009900990099\n","Std of rewards per episode:               35.696325087362595\n","Time elapsed:                             0.35 mins\n","KL between old and new distribution:      0.009997745\n","Entropy:                                  1.0753624\n","Surrogate loss:                           248.6775\n","\n","********** Iteration 2 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 203\n","Average sum of rewards per episode:       -490.0882352941176\n","Std of rewards per episode:               38.85268884156085\n","Time elapsed:                             0.70 mins\n","KL between old and new distribution:      0.009990142\n","Entropy:                                  1.077909\n","Surrogate loss:                           246.54564\n","\n","********** Iteration 3 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 312\n","Average sum of rewards per episode:       -458.3669724770642\n","Std of rewards per episode:               75.65586748778655\n","Time elapsed:                             1.05 mins\n","KL between old and new distribution:      0.009989533\n","Entropy:                                  1.0557387\n","Surrogate loss:                           235.23341\n","\n","********** Iteration 4 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 436\n","Average sum of rewards per episode:       -402.508064516129\n","Std of rewards per episode:               90.95510200161874\n","Time elapsed:                             1.40 mins\n","KL between old and new distribution:      0.009991516\n","Entropy:                                  1.0244951\n","Surrogate loss:                           211.16302\n","\n","********** Iteration 5 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 601\n","Average sum of rewards per episode:       -302.05454545454546\n","Std of rewards per episode:               75.71991888335909\n","Time elapsed:                             1.76 mins\n","KL between old and new distribution:      0.0099912705\n","Entropy:                                  0.9995527\n","Surrogate loss:                           159.92595\n","\n","********** Iteration 6 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 820\n","Average sum of rewards per episode:       -227.31963470319636\n","Std of rewards per episode:               57.27118959701663\n","Time elapsed:                             2.11 mins\n","KL between old and new distribution:      0.009993479\n","Entropy:                                  0.96555036\n","Surrogate loss:                           120.31564\n","\n","********** Iteration 7 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 1093\n","Average sum of rewards per episode:       -182.15384615384616\n","Std of rewards per episode:               42.18733462967077\n","Time elapsed:                             2.47 mins\n","KL between old and new distribution:      0.009992011\n","Entropy:                                  0.915675\n","Surrogate loss:                           95.62546\n","\n","********** Iteration 8 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 1411\n","Average sum of rewards per episode:       -156.24213836477986\n","Std of rewards per episode:               45.37697984473969\n","Time elapsed:                             2.82 mins\n","KL between old and new distribution:      0.00998025\n","Entropy:                                  0.8900422\n","Surrogate loss:                           84.31635\n","\n","********** Iteration 9 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 1737\n","Average sum of rewards per episode:       -152.37730061349694\n","Std of rewards per episode:               39.402072133567216\n","Time elapsed:                             3.18 mins\n","KL between old and new distribution:      0.009999822\n","Entropy:                                  0.84490615\n","Surrogate loss:                           80.99349\n","\n","********** Iteration 10 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 2101\n","Average sum of rewards per episode:       -136.36813186813185\n","Std of rewards per episode:               38.12179261700923\n","Time elapsed:                             3.53 mins\n","KL between old and new distribution:      0.00999538\n","Entropy:                                  0.7757163\n","Surrogate loss:                           73.23879\n","\n","********** Iteration 11 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 2493\n","Average sum of rewards per episode:       -126.56377551020408\n","Std of rewards per episode:               51.16535831857304\n","Time elapsed:                             3.89 mins\n","KL between old and new distribution:      0.00999362\n","Entropy:                                  0.73735166\n","Surrogate loss:                           73.26705\n","\n","********** Iteration 12 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 2923\n","Average sum of rewards per episode:       -115.28139534883721\n","Std of rewards per episode:               27.77412846529104\n","Time elapsed:                             4.24 mins\n","KL between old and new distribution:      0.009991566\n","Entropy:                                  0.7002954\n","Surrogate loss:                           60.79928\n","\n","********** Iteration 13 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 3366\n","Average sum of rewards per episode:       -111.86907449209933\n","Std of rewards per episode:               28.352680761559593\n","Time elapsed:                             4.60 mins\n","KL between old and new distribution:      0.009996893\n","Entropy:                                  0.6532492\n","Surrogate loss:                           59.250095\n","\n","********** Iteration 14 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 3816\n","Average sum of rewards per episode:       -110.11555555555556\n","Std of rewards per episode:               34.158128725454084\n","Time elapsed:                             4.95 mins\n","KL between old and new distribution:      0.009988666\n","Entropy:                                  0.65356314\n","Surrogate loss:                           60.10874\n","\n","********** Iteration 15 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 4261\n","Average sum of rewards per episode:       -111.36404494382022\n","Std of rewards per episode:               41.59923899258285\n","Time elapsed:                             5.31 mins\n","KL between old and new distribution:      0.009985769\n","Entropy:                                  0.6302584\n","Surrogate loss:                           63.18214\n","\n","********** Iteration 16 ************\n","Rollout\n","Made rollout\n","Total number of episodes:                 4729\n","Average sum of rewards per episode:       -105.83974358974359\n","Std of rewards per episode:               23.520993567360037\n","Time elapsed:                             5.66 mins\n","KL between old and new distribution:      0.009979783\n","Entropy:                                  0.6041639\n","Surrogate loss:                           55.27409\n","\n","********** Iteration 17 ************\n","Rollout\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-4e6dbda4efe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Generating paths.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rollout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Made rollout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-1387065954a1>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(env, agent, max_pathlength, n_timesteps)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mobervation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_pathlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobervation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mobervations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobervation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"dgq5wRk3hbAK"},"source":["# Homework option I: better sampling (10+pts)\n","\n","In this section, you're invited to implement a better rollout strategy called _vine_.\n","\n","![img](https://s17.postimg.cc/i90chxgvj/vine.png)\n","\n","In most gym environments, you can actually backtrack by using states. You can find a wrapper that saves/loads states in [the mcts seminar](https://github.com/yandexdataschool/Practical_RL/blob/master/week10_planning/seminar_MCTS.ipynb).\n","\n","You can read more about in the [TRPO article](https://arxiv.org/abs/1502.05477) in section 5.2.\n","\n","The goal here is to implement such rollout policy (we recommend using tree data structure like in the seminar above).\n","Then you can assign cummulative rewards similar to `get_cummulative_rewards`, but for a tree.\n","\n","__bonus task__ - parallelize samples using multiple cores"]},{"cell_type":"markdown","metadata":{"id":"QxPJNAFEhbAK"},"source":["# Homework option II (10+pts)\n","\n","Let's use TRPO to train evil robots! (pick any of two)\n","* [MuJoCo robots](https://gym.openai.com/envs#mujoco)\n","* [Box2d robot](https://gym.openai.com/envs/BipedalWalker-v2)\n","\n","The catch here is that those environments have continuous action spaces. \n","\n","Luckily, TRPO is a policy gradient method, so it's gonna work for any parametric $\\pi_\\theta(a|s)$. We recommend starting with gaussian policy:\n","\n","$$\\pi_\\theta(a|s) = N(\\mu_\\theta(s),\\sigma^2_\\theta(s)) = {1 \\over \\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) } } e^{ (a - \n","\\mu_\\theta(s))^2 \\over 2 {\\sigma^2}_\\theta(s) } $$\n","\n","In the $\\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) }$ clause, $\\pi$ means ~3.1415926, not agent's policy.\n","\n","This essentially means that you will need two output layers:\n","* $\\mu_\\theta(s)$, a dense layer with linear activation\n","* ${\\sigma^2}_\\theta(s)$, a dense layer with activation tf.exp (to make it positive; like rho from bandits)\n","\n","For multidimensional actions, you can use fully factorized gaussian (basically a vector of gaussians).\n","\n","__bonus task__: compare performance of continuous action space method to action space discretization"]},{"cell_type":"code","metadata":{"id":"qHPJGLHchbAL"},"source":[""],"execution_count":null,"outputs":[]}]}