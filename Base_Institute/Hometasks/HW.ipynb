{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 $-$ to import and check visualization of 'fashion-mnist' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/al_cher/Documents/fashion-mnist/data/fashion/' #your way to work_dir\n",
    "def open_fashion_mnist(data_path = data_path):\n",
    "    \n",
    "    #retrieving dims\n",
    "    fd = open(os.path.join(data_path, f'train-images-idx3-ubyte'), 'rb')\n",
    "    load_dims_train = np.fromfile(file = fd, dtype = np.uint32).byteswap(inplace = True)\n",
    "    tlength, trows, tcolumns = load_dims_train[1], load_dims_train[2].astype(np.uint8), load_dims_train[3].astype(np.uint8)\n",
    "    fd = open(os.path.join(data_path, f't10k-images-idx3-ubyte'), 'rb')\n",
    "    load_dims_test = np.fromfile(file = fd, dtype = np.uint32).byteswap(inplace = True)\n",
    "    slength, srows, scolumns = load_dims_test[1], load_dims_test[2].astype(np.uint8), load_dims_test[3].astype(np.uint8)\n",
    "\n",
    "    fd = open(os.path.join(data_path, f'train-images-idx3-ubyte'), 'rb')\n",
    "    loaded = np.fromfile(file = fd, dtype = np.uint8).byteswap(inplace = True)\n",
    "    X_train = loaded[16:].reshape((tlength, trows, tcolumns)).astype(float)\n",
    "    \n",
    "    fd = open(os.path.join(data_path, f't10k-images-idx3-ubyte'), 'rb')\n",
    "    loaded = np.fromfile(file = fd, dtype = np.uint8).byteswap(inplace = True)\n",
    "    X_test = loaded[16:].reshape((slength, srows, scolumns)).astype(float)\n",
    "       \n",
    "    fd = open(os.path.join(data_path, f'train-labels-idx1-ubyte'), 'rb')\n",
    "    loaded = np.fromfile(file = fd, dtype = np.uint8).byteswap(inplace = True)\n",
    "    Y_train = np.asarray(loaded[8:].reshape((tlength)))\n",
    "    \n",
    "    fd = open(os.path.join(data_path, f't10k-labels-idx1-ubyte'), 'rb')\n",
    "    loaded = np.fromfile(file = fd, dtype = np.uint8).byteswap(inplace = True)\n",
    "    Y_test = np.asarray(loaded[8:].reshape((slength)))\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = open_fashion_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Digits:  0 1 2 3 4 5 6 7 8 9')\n",
    "print('labels: %s' % np.unique(Y_train))\n",
    "print('Train classes distribution: %s' % np.bincount(Y_train[:]))\n",
    "print('Test classes distribution: %s' % np.bincount(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    " 0: \"T-shirt'/'top\",\n",
    " 1: \"Trouser\",\n",
    " 2: \"Pullover\",\n",
    " 3: \"Dress\",\n",
    " 4: \"Coat\",\n",
    " 5: \"Sandal\",\n",
    " 6: \"Shirt\",\n",
    " 7: \"Sneaker\",\n",
    " 8: \"Bag\",\n",
    " 9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "def imshow(X, Y):\n",
    "    for idx, im in enumerate(X):\n",
    "        print(label_dict[Y[idx - 1]])\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "_ = imshow(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 − SVD decomposition starters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition and intuitive interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Singular Value Decomposition (SVD)** of matrix $A$ is the factorization of $A$ into  the product of three matrices $A = UDV^T$ where the columns of $U$ and $V$ are orthonormal and the matrix $D$ is diagonal with positive real entries. \n",
    "\n",
    "Let $A$ be an $n \\times d$ matrix with singular vectors $\\mathbf{v_1}, ... , \\mathbf{v_r}$ and corresponding values $\\sigma_1, ..., \\sigma_r$: \n",
    "                    \\begin{equation*} \n",
    "                       \\mathbf{v_j} = argmax_{\\mathbf{v_j} \\perp \\mathbf{v_1}, ..., \\mathbf{v_{j-1}}}|A\\mathbf{v}|\n",
    "                       \\\\ \\sigma_j = |A\\mathbf{v_j}|\n",
    "                    \\end{equation*}\n",
    "Then $\\mathbf{u_j}$ are left-singular vectors, according to linear algebra, $\\mathbf{u_j} = \\frac{1}{\\sigma_j}A\\mathbf{v_j}$ Thus, \n",
    "                    \\begin{equation*} \n",
    "                      A = \\sum_{i}^{r} \\sigma_i \\mathbf{u_i} \\mathbf{v_i^T}\n",
    "                    \\end{equation*}\n",
    "    *About dims*: $A_{n \\times d} = U_{n \\times r} D_{r \\times r} V_{r \\times d}^T $\n",
    "                    \n",
    "**Geometrical meaning**\n",
    "\n",
    "\n",
    "Let a linear operator be associated with the matrix $A$. The singular decomposition can be reformulated in geometric terms. A linear operator that displays the elements of space in the form $\\mathfrak{R}^n$ of executable linear operators of rotation, extension, and rotation. Thus, singular changes in the display of geometric changes when displaying a linear operator are all in different spaces.\n",
    "\n",
    "<img src = \"svd_depict.png\">\n",
    "\n",
    "In the special case when $M$ is an $m \\times m$ real square matrix, the matrices $U$ and $V^*$ can be chosen to be real $m \\times m$ matrices too.\n",
    "    $M^* Mv = \\sigma u$, $MM^* u = \\sigma v$\n",
    "\n",
    "In that case, \"unitary\" is the same as \"orthonormal\". Then, interpreting each such matrix A as the linear transformation $x \\rightarrow Ax$ of the space $R^m$, the matrices $U$ and $V^*$ represent rotations or reflection of the space, while $D$ represent the scaling of each coordinate $x_i$ by the factor $σ_i$. Thus the SVD decomposition breaks down any invertible linear transformation of $R^m$ into a composition of three geometrical transformations: a rotation or reflection ($V^*$), followed by a coordinate-by-coordinate scaling ($D$), followed by another rotation or reflection ($U$).\n",
    "\n",
    "In particular, if $M$ has a positive determinant, then $U$ and $V^*$ can be chosen to be both reflections, or both rotations. If the determinant is negative, exactly one of them will have to be a reflection. If the determinant is zero, each can be chosen to be of each type, independently.\n",
    "\n",
    "If the matrix $M$ is real but not square, namely $m \\times n$ with $m ≠ n$, it can be interepreted as a linear transformation from $R^n$ to $R^m$. Then $U$ and $V^*$ can be chosen to be rotations of $^Rm$ and $R^n$, respectively; and $D$, besides scaling the first *min{m, n}* coordinates, also extends the vector with zeros, or removes trailing coordinates, so as to turn $R^n$ into $R^m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[More in Russian](http://www.machinelearning.ru/wiki/index.php?title=%D0%A1%D0%B8%D0%BD%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5) \\\\\n",
    "[More in English](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/data $-$ here i copied the task to show how SVD helps in dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.*\n",
    "\n",
    "**File descriptions**: \n",
    "\n",
    "-- *labeledTrainData* $-$ The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
    "\n",
    "-- *testData* $-$ The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. \n",
    "\n",
    "-- *unlabeledTrainData* $-$ An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review. \n",
    "\n",
    "**Data fields**: \n",
    "\n",
    "-- *id* $-$ Unique ID of each review\n",
    "\n",
    "-- *sentiment* $-$ Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "\n",
    "-- *review* $-$ Text of the review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is  (25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing data\n",
    "import pandas as pd\n",
    "imdb = pd.read_csv('labeledTrainData.tsv', delimiter='\\t')\n",
    "print(\"Dataset shape is \", imdb.shape)\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's show that dataset has well-balanced sentiments' classes distribution\n",
    "imdb.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making train and test split and vectoring input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(imdb.review.values, imdb.sentiment.values)\n",
    "vect = TfidfVectorizer(sublinear_tf = True, use_idf = True)\n",
    "X_train = vect.fit_transform(x_train)\n",
    "X_test = vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.81408\n",
      "ROC-AUC:  0.8995751463752784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "clf = LogisticRegression(C = 0.15, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "print (\"Accuracy: \", metrics.accuracy_score(y_test, clf.predict(X_test)))\n",
    "print (\"ROC-AUC: \", metrics.roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object dim of X_test is  6250\n",
      "Feature dim of X_test is  66788\n"
     ]
    }
   ],
   "source": [
    "#let's look at dimension of X_test, e.g\n",
    "print(\"Object dim of X_test is \", X_test.shape[0])\n",
    "print(\"Feature dim of X_test is \", X_test.shape[1])\n",
    "#so we can see that dimension of feature space is extremely higher than object space!\n",
    "# => to increase accuracy (now it's potentially low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning classification problems, there are often too many factors on the basis of which the final classification is done. These factors are basically variables called features. The higher the number of features, the harder it gets to visualize the training set and then work on it. Sometimes, most of these features are correlated, and hence redundant. This is where dimensionality reduction algorithms come into play. Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Truncated SVD*: This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.86032\n",
      "ROC-AUC:  0.9381474399468904\n"
     ]
    }
   ],
   "source": [
    "#implementing PCA using trucnated SVD \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "tsvd = TruncatedSVD(n_components = 100)\n",
    "X_train_pca = tsvd.fit_transform(X_train)\n",
    "X_test_pca = tsvd.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_pca, y_train)\n",
    "print (\"Accuracy: \", metrics.accuracy_score(y_test, clf.predict(X_test_pca)))\n",
    "print (\"ROC-AUC: \", metrics.roc_auc_score(y_test, clf.predict_proba(X_test_pca)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features that the principal component method provides are optimal for linear methods, so logistic regression shows better results than complex non-linear classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 $-$ Misses, NaNs in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by [Zen.AI](https://www.zest.ai/blog/6-methods-for-dealing-with-missing-data) blog\n",
    "\n",
    "1)**Encode NAs as -1 or -9999**. This works reasonably well for numerical features that are predominantly positive  in value, and for tree-based models in general. This used to be a more common method in the past when the out-of-the box machine learning libraries and algorithms were not very adept at working with missing data.\n",
    "    \n",
    "2)**Casewise deletion of missing data**. Here you simply drop all cases or rows from the dataset that contain missing values. In the case of a very large dataset with very few missing values, this approach could potentially work really well. However, if the missing values are in cases that are also otherwise statistically distinct, this method may seriously skew the predictive model for which this data is used. Another major problem with this approach is that it will be unable to process any future data that contains missing values. If your predictive model is designed for production, this could create serious issues in deployment.\n",
    "    \n",
    "3)**Replace missing values with the mean/median value of the feature in which they occur**. This works for numerical features. The choice of median/mean is often related to the form of distribution that the data has. For imbalanced data, the median may be more appropriate, while for symmetrical and more normally distributed data, the mean could be a better choice.\n",
    "    \n",
    "4)**Label encode NAs as another level of a categorical variable**. This works with tree-based models and other models if the feature can be numerically transformed (one-hot encoding, frequency encoding, etc.). This technique does not work well with logistic regression.\n",
    "    \n",
    "5)**Run predictive models that impute the missing data**. This should be done in conjunction with some kind of cross-validation scheme in order to avoid leakage. This can be very effective and can help with the final model.\n",
    "    \n",
    "6)**Use the number of missing values in a given row to create a new engineered feature**. As mentioned above, missing data can often have lots of useful signal in its own right, and this is a good way to encode that information.\n",
    "\n",
    "\n",
    "Handling w\\ missing data $ - $ (authot's rights reverved by copying from [there](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4))\n",
    "<img src = 'missing_data.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1309 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    1309 non-null int64\n",
      "Pclass         1309 non-null int64\n",
      "Name           1309 non-null object\n",
      "Sex            1309 non-null object\n",
      "Age            1046 non-null float64\n",
      "SibSp          1309 non-null int64\n",
      "Parch          1309 non-null int64\n",
      "Ticket         1309 non-null object\n",
      "Fare           1308 non-null float64\n",
      "Cabin          295 non-null object\n",
      "Embarked       1307 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 122.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#let's import trivial dataset Titanic and concatenate train and test parts: not so exciting to fit classifier \n",
    "#but to feek NaNs of misses\n",
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "N_shape = len(train)\n",
    "y = train['Survived']\n",
    "dataset = pd.concat(objs = [train.drop(columns = ['Survived']), test], axis = 0)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabin          1014\n",
      "Age             263\n",
      "Embarked          2\n",
      "Fare              1\n",
      "Ticket            0\n",
      "Parch             0\n",
      "SibSp             0\n",
      "Sex               0\n",
      "Name              0\n",
      "Pclass            0\n",
      "PassengerId       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#getting info about missed values\n",
    "total = dataset.isnull().sum().sort_values(ascending = False)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>1307</td>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>1308</td>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>1309</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                               Name  \\\n",
       "0              1       3                            Braund, Mr. Owen Harris   \n",
       "1              2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2              3       3                             Heikkinen, Miss. Laina   \n",
       "3              4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4              5       3                           Allen, Mr. William Henry   \n",
       "..           ...     ...                                                ...   \n",
       "413         1305       3                                 Spector, Mr. Woolf   \n",
       "414         1306       1                       Oliva y Ocana, Dona. Fermina   \n",
       "415         1307       3                       Saether, Mr. Simon Sivertsen   \n",
       "416         1308       3                                Ware, Mr. Frederick   \n",
       "417         1309       3                           Peter, Master. Michael J   \n",
       "\n",
       "        Sex  SibSp  Parch              Ticket  \n",
       "0      male      1      0           A/5 21171  \n",
       "1    female      1      0            PC 17599  \n",
       "2    female      0      0    STON/O2. 3101282  \n",
       "3    female      1      0              113803  \n",
       "4      male      0      0              373450  \n",
       "..      ...    ...    ...                 ...  \n",
       "413    male      0      0           A.5. 3236  \n",
       "414  female      0      0            PC 17758  \n",
       "415    male      0      0  SOTON/O.Q. 3101262  \n",
       "416    male      0      0              359309  \n",
       "417    male      1      1                2668  \n",
       "\n",
       "[1309 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st thing: we can drop this values\n",
    "# e.g., like this\n",
    "inplace = False\n",
    "dataset.dropna(how = 'all', inplace = inplace)\n",
    "#column-wise\n",
    "dataset.dropna(axis = 1, inplace = inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -99\n",
       "1       C85\n",
       "2       -99\n",
       "3      C123\n",
       "4       -99\n",
       "       ... \n",
       "413     -99\n",
       "414    C105\n",
       "415     -99\n",
       "416     -99\n",
       "417     -99\n",
       "Name: Cabin, Length: 1309, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#or fill with some extra value to mark\n",
    "dataset.Cabin.fillna(-99, inplace = inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      22.000000\n",
       "1      38.000000\n",
       "2      26.000000\n",
       "3      35.000000\n",
       "4      35.000000\n",
       "         ...    \n",
       "886    27.000000\n",
       "887    19.000000\n",
       "888    27.915709\n",
       "889    26.000000\n",
       "890    32.000000\n",
       "Name: Age, Length: 891, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fill with median/average/mode\n",
    "dataset.Age.fillna(dataset.Age.median(), inplace = inplace)\n",
    "#or like this with groupby\n",
    "train['Age'].fillna(train.groupby('Sex')['Age'].transform(\"mean\"), inplace = inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from data distribution\n",
    "dataset.Age.hist(bins = 5)\n",
    "nan_list = np.random.randint(0, high = 80, size = len(dataset), dtype='l')\n",
    "dataset['Age'][np.isnan(dataset['Age'])] = nan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# to train a linear regression\n",
    "# with data like this -- only for continous mesurable data\n",
    "array = train[['Pclass','SibSp','Parch','Fare','Age']]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_train = array[array['Age'].notnull()].drop(columns = 'Age', inplace = False)\n",
    "y_train = array[array['Age'].notnull()]['Age']\n",
    "X_test = array[array['Age'].isnull()].drop(columns = 'Age', inplace = False)\n",
    "y_test = array[array['Age'].isnull()]['Age']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "array.Age[array.Age.isnull()] = predicted\n",
    "train.describe()\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#using KNN -- for continous and categorical data -- with 5 default neighbours\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "columns = list(train)\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "array.Age[train.Age.isnull()] = predicted\n",
    "train.describe()\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional extras (Only theoretical without practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Series Specific Methods\n",
    "\n",
    "-- **Last Observation Carried Forward (LOCF) & Next Observation Carried Backward (NOCB)**\n",
    "\n",
    "   This is a common statistical approach to the analysis of longitudinal repeated measures data where some follow-up observations may be missing. Longitudinal data track the same sample at different points in time. Both these methods can introduce bias in analysis and perform poorly when data has a visible trend\n",
    "   \n",
    "-- **Linear Interpolation**\n",
    "\n",
    "   This method works well for a time series with some trend but is not suitable for seasonal data\n",
    "    \n",
    "-- **Seasonal Adjustment + Linear Interpolation**\n",
    "\n",
    "   This method works well for data with both trend and seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import impyute as na\n",
    "# na.random(dataset)                  # Random Imputation\n",
    "# na.locf(dataset, option = \"locf\")   # Last Obs. Carried Forward\n",
    "# na.locf(dataset, option = \"nocb\")   # Next Obs. Carried Backward\n",
    "# na.interpolation(dataset)           # Linear Interpolation\n",
    "# na.seadec(dataset, algorithm = \"interpolation\") # Seasonal Adjustment then Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
