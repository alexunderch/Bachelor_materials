{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent and recursive neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть первая: определение реккурентных и рекурсивных нейросетей. Разница между подходами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реккурентная нейронная сеть (Recurrent Neural Network)** $-$ вид нейронной сети, где связи между элементами образуют линейную напраленную последовательность, благодаря чему появляется возможность обрабатывать серии событий во времени или последовательные пространственные цепочки. В отличие от многослойных сетей других видов, например, свёрточных или fully-connected, такое устройство нейростети позволяет использовать её для обратботки последовательностей произвольной длины.\n",
    "\n",
    "Особенности устройства:\n",
    "<img src = 'rnn.jpg'>\n",
    "\n",
    "Обозначения:\n",
    "\n",
    "* $x_t$ $-$ вход сети в каждый момент времени $t$;\n",
    "* $s_t$ $-$ \"скрытое\" состояние в каждый момент времени. Оно характеризует возможность сети \"запоминать своё состояние\" и вычислять новое на основе состояния в предыдущий момент времени и входного состояния в текущий момент  времени: $s_t = f(Ux_t + Ws_{t-1} + bias)$, где $U, W$ $-$ соответствующие матрицы весов, $f$ $-$ какая-то функция активации. $s_{-1}$ обычно инициализируется, например, нулями или очень маленькими значениями;\n",
    "* $o_t$ $-$ выход сети в каждый момент времени. Например, если мы хотим предсказать вероятность каждого выходного слова в сгенерированной из словаря последовательности, то $o_t = softmax(Vs_t)$.\n",
    "\n",
    "Примечание:\n",
    "\n",
    "В отличие от обычной (например, fully-connected) сети, матрицы параметров $U, W, V$ общие по всей сети, что сильно уменьшает число обучаемых параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Рекурсивная нейронная сеть (Recursive Neural Network, RvNN)** $-$ так же, как и рекуррентные сети, могут работать с данными переменной длины, но могут и учитывать более сложные особенности данных $-$ такие, как их иерархическая структура. $<<$ В рекурсивных сетях нейроны с одинаковыми весами активируются рекурсивно в соответствии со структурой сети. В процессе работы рекурсивной сети вырабатывается модель для предсказания для структур переменной размерности, так и скалярных структур через активацию структуры в соответствии с топологией, $>>$ $-$ согласно материалам Википедии\n",
    "\n",
    "Общее понятие об архитектуре:\n",
    "\n",
    "<img src = \"rvnn.png\" width = \"200\" height = \"200\">\n",
    "\n",
    "Родительские векторы должны иметь одинаковую размерность, чтобы быть рекурсивно совместимыми и использоваться в качестве входных данных для следующей композиции.\n",
    "Последующие шаги получают на вход меру предыдущего корня и следующее слово последовательности, таким образом пока в сети не будет сформировано дерево со всеми словами в последовательности.\n",
    "Деревья могут иметь разную структуру, выбор лучшей подструктуры дерева для сети основывается на их мере. Мера дерева - сумма мер на каждом узле.\n",
    "\n",
    "$p_{1,2} = f(W[c_1; c_2] + bias)$, где $W$ $-$ матрица весов, *\\[\\]* обозначает операцию *hstack()*\n",
    "Так же, как и для классификации, $s = softmax(W_{score} p_{1,2})$\n",
    "\n",
    "Рекуррентная нейронная сеть представляет собой рекурсивную сеть со специфической структурой - в виде линейной цепочки. Рекурсивные сети работают на структурах общего типа, включающих иерархию, рекуррентные сети работают исключительно на линейной прогрессии во времени, связывая предыдущий момент времени со следующим через скрытый нейронный слой.\n",
    "\n",
    "\n",
    "Об алгоритме обратного распространения ошибки ([отсюда](http://neerc.ifmo.ru/wiki/index.php?title=%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%81%D0%B8%D0%B2%D0%BD%D1%8B%D0%B5_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8)):\n",
    "*     Сумма производных матрицы W от всех узлов. Можно предположить, что она разная на каждом узле, однако если взять отдельные производные от каждого вхождения, то получится то же самое.\n",
    "*    Разделение производных в каждом узле. Во время прямого распространения, родительский вектор считается через дочерние узлы по формуле выше. Следовательно, ошибки должны быть вычислены относительно каждого из них, причём ошибка каждого дочернего узла является n-мерной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представление слоя модели в виде тензора:\n",
    "<img src = \"tensor_nn.png\">\n",
    "Великолепная [статья](https://arxiv.org/pdf/1403.4462.pdf) про применение тензоров в анализе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнительный анализ между RNN, RvNN и TensorNN [редактор таблиц](https://www.tablesgenerator.com/markdown_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Название архитектуры   | Плюсы                                                                                                                                                                                                                                                                                                         | Возможности                                                                                                                               | Ограничения                                                                                                                                                                                                                                                                                                                                          |\n",
    "|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| RNN                    | 1. Малое число обучаемых параметров 2. Parameter-sharing 3. Возможность подстраиваться под  конкретные особенности следования данных 4. Наличие гибких attention-механизмов 5. Любая динамическая система может быть  с любой точностью описана рекуррентной  нейронной сетью                                 |  NLP signal processing machine translation анализ сложных закономерностей последовательностей данных                                      | 1. Сложность совмещения масштабов 2. Градиентные затухания и взрывы 3. Сложность алгоритмов обратного  распространения ошибки 4. Проблема длинных контестов (решение: LSTM)                                                                                                                                                                          |\n",
    "| RvNN                   | 1. хорошее выделение субструктрур данных, их иерархическое упорядочение 2. parameter-sharing типа \"родитель-дети\" -> сокращение числа обучаемых параметров 3. дерево решений можно визуализировать для лучшего понимания структуры данных 4. Обучение сети методом рекурсивного обхода дерева (также и минус) | machine translation - самая  популярная сфера применения из-за структуры языка; анализ пейзажных композиций на пример стуктуры ландшафта; | 1. сильное ограничение возможных отношений порядка: RNN как комбинация RvNN есть более  эффективная структура для поиска разнообразной структуры 2. Бо'льшее, чем у RNN, число параметров  3. Рекурсивный обход сложных сетей сильно увел. время обучения и обращения к памяти - можно и компьютер убить. 4. Быстрое затухание градиентов при спуске |\n",
    "| RvNN с тензорным слоем | {RVNN} + бо'льшие возможные размерности  данных                                                                                                                                                                                                                                                                | можно добавить сентимент-анализ и аннотирование                                                                                           |  те же, что и у RvNN, но, думаю, время вычисления можно сократить обращением к TPU-процессорам                                                                                                                                                                                                                                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LSTM model on the IMDB sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
